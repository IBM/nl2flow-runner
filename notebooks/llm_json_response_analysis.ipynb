{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# write jsonl file\n",
    "from ai_profiling.generators.planning_data_generator_datatypes import ValidationLLMResponseJsonData\n",
    "from ai_profiling.helpers.file_helper.file_helper import get_base_model_from_json, get_files_in_folder\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "folder_path = \"/Users/jungkookang/Downloads/nl2flow/output/translation\"\n",
    "file_paths = get_files_in_folder(folder_path=Path(folder_path), file_extension=\"json\")\n",
    "\n",
    "llm_response_json_evaluation_data = []\n",
    "for file_path in file_paths:\n",
    "    llm_response_json_evaluation_datum = get_base_model_from_json(\n",
    "        file_path=file_path, base_model=ValidationLLMResponseJsonData\n",
    "    )\n",
    "    llm_response_json_evaluation_data.append(llm_response_json_evaluation_datum)\n",
    "\n",
    "\n",
    "len(llm_response_json_evaluation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all data to each llm model\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import statistics\n",
    "from typing import Dict, List\n",
    "\n",
    "# [total, correct]\n",
    "llm_bin: Dict[str, Dict[str, int]] = defaultdict(\n",
    "    lambda: {\n",
    "        \"total\": 0, \n",
    "        \"perfect\": 0, \n",
    "        \"parsing\": 0, \n",
    "        \"num_hallucination_per_problem\": [], \n",
    "        \"num_missing_element_per_problem\": []\n",
    "        }\n",
    ")\n",
    "model_ids_failed = []\n",
    "for llm_response_json_evaluation_unit in llm_response_json_evaluation_data:\n",
    "    llm_model_id = (llm_response_json_evaluation_unit.llm_response_planning_data.llm_response.llm_model_id).split(\"/\")[-1]\n",
    "    llm_model_id = llm_model_id.replace(\"3-1\", \"3.1\").replace(\"3-3\", \"3.3\")\n",
    "\n",
    "    is_perfect_translation = True\n",
    "    total_total = 0\n",
    "    total_correct = 0\n",
    "    total_hallucination = 0\n",
    "    total_missing = 0\n",
    "    llm_bin[llm_model_id][\"total\"] += 1\n",
    "    # check if translation is intact\n",
    "    stat = llm_response_json_evaluation_unit.json_translation_stat\n",
    "\n",
    "    if stat is None:\n",
    "        continue\n",
    "\n",
    "    llm_bin[llm_model_id][\"parsing\"] += 1\n",
    "\n",
    "    for field in stat.model_fields_set:\n",
    "        field_obj = getattr(stat, field)\n",
    "        if field_obj.num_correct != field_obj.total:\n",
    "            is_perfect_translation = False\n",
    "\n",
    "        total_total += field_obj.total\n",
    "        total_correct += field_obj.num_correct\n",
    "        total_hallucination += field_obj.num_hallucination\n",
    "        total_missing += field_obj.num_missing\n",
    "    \n",
    "    llm_bin[llm_model_id][\"num_hallucination_per_problem\"].append(total_hallucination)\n",
    "    llm_bin[llm_model_id][\"num_missing_element_per_problem\"].append(total_missing)\n",
    "    \n",
    "\n",
    "    if is_perfect_translation:\n",
    "        llm_bin[llm_model_id][\"perfect\"] += 1\n",
    "\n",
    "\n",
    "# calculate average\n",
    "for llm_model_id in llm_bin.keys():\n",
    "    lst = llm_bin[llm_model_id][\"num_hallucination_per_problem\"]\n",
    "    llm_bin[llm_model_id][\"mean_num_hallucination_per_problem\"] = statistics.mean(lst) if len(lst) > 0 else -1.0\n",
    "    if llm_bin[llm_model_id][\"mean_num_hallucination_per_problem\"] < 0.0:\n",
    "        model_ids_failed.append(llm_model_id)\n",
    "        print(f\"No valid response for {llm_model_id}\")\n",
    "    llm_bin[llm_model_id][\"std_num_hallucination_per_problem\"] = statistics.stdev(lst) if len(lst) > 0 else -1.0\n",
    "    \n",
    "    lst_0 = llm_bin[llm_model_id][\"num_missing_element_per_problem\"]\n",
    "    llm_bin[llm_model_id][\"mean_num_missing_element_per_problem\"] = statistics.mean(lst_0) if len(lst_0) > 0 else -1.0\n",
    "    llm_bin[llm_model_id][\"std_num_missing_element_per_problem\"] = statistics.stdev(lst_0) if len(lst_0) > 0 else -1.0\n",
    "\n",
    "for model_id in model_ids_failed:\n",
    "    del llm_bin[llm_model_id]\n",
    "\n",
    "pprint.pp(llm_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from math import ceil\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_planning_chart_multi_rev(llm_bin_input, file_name_path: str, row_labels: List[str]) -> None:\n",
    "    llm_bin2 = deepcopy(llm_bin_input)\n",
    "    rowmap = {row_name: idx for idx, row_name in enumerate(row_labels)}\n",
    "    rows = int(ceil(len(llm_bin2) / 2))\n",
    "    fig, ax = plt.subplots(rows, 2, figsize=(8, 6))\n",
    "    fig.tight_layout()\n",
    "    handles = None\n",
    "    labels = None\n",
    "    counter = 0\n",
    "    for quality, obj in llm_bin2.items():\n",
    "        col = counter % 2\n",
    "        row = counter // 2\n",
    "        width = 0.25  # the width of the bars\n",
    "        multiplier = 0\n",
    "        model_names = list(obj.keys())\n",
    "        model_names.sort()\n",
    "        values = [obj[model_name] for model_name in model_names]\n",
    "        x = np.arange(1)  # the label locations\n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            offset = width * multiplier\n",
    "            \n",
    "            rects = ax[row, col].bar(x + offset, [(values[idx] if values[idx] >= 0.0 else 0.0)], width * 0.8, label=model_name.capitalize())\n",
    "            if values[idx] >= 0.0:\n",
    "                ax[row, col].bar_label(rects, padding=3, fontsize=10, fmt=\"%.2f\")\n",
    "            multiplier += 1\n",
    "\n",
    "        # if row == 0:\n",
    "        #     ax[row].set_title(f\"{prompt_type.capitalize()} prompt\", y=1, fontsize=12)\n",
    "        if \"number\" not in quality:\n",
    "            ax[row, col].set_ylim(0, 1.0)\n",
    "        ax[row, col].xaxis.set_visible(False)\n",
    "        # ax[row, col].tick_params(axis='x', labelsize=12)\n",
    "        ax[row, col].tick_params(axis=\"y\", labelsize=12)\n",
    "        # ax[row, col].set_xlabel(\"Plan length\", fontsize=12)\n",
    "        ax[row, col].set_ylabel(f\"{quality}\", fontsize=12)\n",
    "        ax[row, col].spines[[\"right\", \"top\"]].set_visible(False)\n",
    "\n",
    "        if row == rows - 1 and col == 1:\n",
    "        # ax[row, col].legend(loc=\"best\",fontsize='small') # bbox_to_anchor=(legend_x, legend_height)\n",
    "            handles, labels = ax[row, col].get_legend_handles_labels()\n",
    "        # ax[row, col].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        counter += 1\n",
    "\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "    fig.legend(handles, labels, bbox_to_anchor=(1.4, 0.7), fontsize=\"large\", loc=\"upper right\", frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_name_path, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def process_model_name(model_name):\n",
    "    new_model_name = model_name.split(\"/\")[-1]\n",
    "    if \"DeepSeek-V3\" == new_model_name:\n",
    "        new_model_name += \"(685b, fp8)\"\n",
    "    \n",
    "    return new_model_name.strip().lower()\n",
    "\n",
    "new_llm_bin = deepcopy(llm_bin)\n",
    "new_llm_bin = {process_model_name(model_name): val for model_name, val in new_llm_bin.items()}\n",
    "\n",
    "ordered_dict = collections.OrderedDict(sorted(new_llm_bin.items(), key=lambda it: str(it[0]), reverse=False)) # collections.OrderedDict(sorted(llm_bin.items()))\n",
    "\n",
    "data_dict = {\n",
    "    \"Successful JSON Parsing Rate\": {model_name: (stat[\"parsing\"] / stat[\"total\"]) for model_name, stat in ordered_dict.items()},\n",
    "    \"Perfect translation rate\": {model_name: (stat[\"perfect\"] / stat[\"total\"]) for model_name, stat in ordered_dict.items()},\n",
    "    \"Mean number of\\n hallucinated elements per problem\": {model_name: stat[\"mean_num_hallucination_per_problem\"] for model_name, stat in ordered_dict.items()},\n",
    "    \"Mean number of\\n missing elements per problem\": {model_name: stat[\"mean_num_missing_element_per_problem\"] for model_name, stat in ordered_dict.items()}\n",
    "}\n",
    "\n",
    "make_planning_chart_multi_rev(\n",
    "    llm_bin_input=data_dict,\n",
    "    file_name_path=\"output/plot/translation_rates.png\",\n",
    "    row_labels = list(data_dict.keys())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "X_model = {}\n",
    "Y_model = {}\n",
    "\n",
    "X_reg = []\n",
    "Y_reg = []\n",
    "X_model_reg = {}\n",
    "Y_model_reg = {}\n",
    "\n",
    "for llm_response_json_evaluation_unit in llm_response_json_evaluation_data:\n",
    "    llm_model_id = llm_response_json_evaluation_unit.llm_response_planning_data.llm_response.llm_model_id\n",
    "    if llm_model_id not in X_model:\n",
    "        X_model[llm_model_id] = []\n",
    "        Y_model[llm_model_id] = []\n",
    "        X_model_reg[llm_model_id] = []\n",
    "        Y_model_reg[llm_model_id] = []\n",
    "\n",
    "    # check if translation is intact\n",
    "    stat = llm_response_json_evaluation_unit.json_translation_stat\n",
    "    is_perfect_translation = True\n",
    "    llm_bin[llm_model_id][\"total\"] += 1\n",
    "\n",
    "    parsing_success = 0\n",
    "    perfect_translation = 0\n",
    "    prop_correct = -1.0\n",
    "    prop_hallucination = -1.0\n",
    "    prop_missing = -1.0\n",
    "    tag = llm_response_json_evaluation_unit.llm_response_planning_data.pddl_generator_output.planning_datum_tag\n",
    "    tmp_x = [\n",
    "        float(tag.number_of_agents),\n",
    "        float(tag.input_parameters_per_agent),\n",
    "        float(tag.coupling_of_agents),\n",
    "        float(\n",
    "            llm_response_json_evaluation_unit.llm_response_planning_data.pddl_generator_output.agent_info_generator_input.proportion_slot_fillable_variables\n",
    "        ),\n",
    "        float(tag.number_of_goals),\n",
    "    ]\n",
    "\n",
    "    # float(tag.length_of_sequence),\n",
    "    # float(len(llm_response_json_evaluation_unit.llm_response_planning_data.planning_prompt)),\n",
    "    # float(len(llm_response_json_evaluation_unit.llm_response_planning_data.llm_response.generated_text)),\n",
    "    if stat is not None:\n",
    "        parsing_success = 1\n",
    "        total_total = 0\n",
    "        total_correct = 0\n",
    "        total_hallucination = 0\n",
    "        total_missing = 0\n",
    "\n",
    "        for field in stat.model_fields_set:\n",
    "            field_obj = getattr(stat, field)\n",
    "\n",
    "            total_total += field_obj.total\n",
    "            total_correct += field_obj.num_correct\n",
    "            total_hallucination += field_obj.num_hallucination\n",
    "            total_missing += field_obj.num_missing\n",
    "\n",
    "            if field_obj.num_correct != field_obj.total:\n",
    "                is_perfect_translation = False\n",
    "\n",
    "        if is_perfect_translation:\n",
    "            llm_bin[llm_model_id][\"perfect\"] += 1\n",
    "\n",
    "        perfect_translation = 1 if is_perfect_translation else 0\n",
    "        prop_correct = total_correct / total_total\n",
    "        prop_hallucination = total_hallucination / total_total\n",
    "        prop_missing = total_missing / total_total\n",
    "\n",
    "        tmp_y_reg = [prop_correct, prop_hallucination, prop_missing]\n",
    "        Y_reg.append(tmp_y_reg)\n",
    "        Y_model_reg[llm_model_id].append(tmp_y_reg)\n",
    "        X_reg.append(tmp_x)\n",
    "        X_model_reg[llm_model_id].append(tmp_x)\n",
    "\n",
    "    tmp_y = [parsing_success, perfect_translation]\n",
    "\n",
    "    Y.append(tmp_y)\n",
    "    Y_model[llm_model_id].append(tmp_y)\n",
    "    X.append(tmp_x)\n",
    "    X_model[llm_model_id].append(tmp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# normalize\n",
    "X_1 = np.array(X, dtype=np.float64)\n",
    "X_n = normalize(X_1, axis=0, norm=\"l2\")\n",
    "\n",
    "# normalize\n",
    "X_1_reg = np.array(X_reg, dtype=np.float64)\n",
    "X_n_reg = normalize(X_1_reg, axis=0, norm=\"l2\")\n",
    "\n",
    "X_model_n = {}\n",
    "for llm_model_id, data_x in X_model.items():\n",
    "    tmp_1 = np.array(data_x, dtype=np.float64)\n",
    "    if len(tmp_1) > 0:\n",
    "        X_model_n[llm_model_id] = normalize(tmp_1, axis=0, norm=\"l2\")\n",
    "\n",
    "X_model_n_reg = {}\n",
    "for llm_model_id, data_x in X_model_reg.items():\n",
    "    tmp_1 = np.array(data_x, dtype=np.float64)\n",
    "    if len(tmp_1) > 0:\n",
    "        X_model_n_reg[llm_model_id] = normalize(tmp_1, axis=0, norm=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose Y\n",
    "parsing_success_idx = 0\n",
    "perfect_translation_idx = 1\n",
    "prop_correct_idx = 0\n",
    "prop_hallucination_idx = 1\n",
    "prop_missing_idx = 2\n",
    "\n",
    "parsing_success_y = np.array(list(map(lambda arr: arr[parsing_success_idx], Y)))\n",
    "perfect_translation_y = np.array(list(map(lambda arr: arr[perfect_translation_idx], Y)))\n",
    "prop_correct_y = np.array(list(map(lambda arr: arr[prop_correct_idx], Y_reg)))\n",
    "prop_hallucination_y = np.array(list(map(lambda arr: arr[prop_hallucination_idx], Y_reg)))\n",
    "prop_missing_y = np.array(list(map(lambda arr: arr[prop_missing_idx], Y_reg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "from ai_profiling.helpers.file_helper.file_helper import write_txt_file\n",
    "\n",
    "\n",
    "output_folder_path = \"output/regression_translation\"\n",
    "group_name = \"parsing_success\"\n",
    "\n",
    "print(\"Parsing Success\")\n",
    "print(\"\\n\\n\\nSubject: Total\")\n",
    "logit = sm.Logit(parsing_success_y, X_n)\n",
    "result = logit.fit_regularized(method=\"l1\", alpha=1)\n",
    "name = \"total\"\n",
    "write_txt_file(file_path=os.path.join(output_folder_path, group_name, (name + \".csv\")), text=result.summary().as_csv())\n",
    "\n",
    "for llm_model_id in X_model_n.keys():\n",
    "    print(f\"\\n\\n\\nSubject: {llm_model_id}\")\n",
    "    x_n = X_model_n[llm_model_id]\n",
    "    y = np.array(list(map(lambda arr: arr[parsing_success_idx], Y_model[llm_model_id])))\n",
    "    logit = sm.Logit(y, x_n)\n",
    "    result = logit.fit_regularized(method=\"l1\", alpha=1)\n",
    "    name = llm_model_id.split(\"/\")[-1]\n",
    "    write_txt_file(\n",
    "        file_path=os.path.join(output_folder_path, group_name, (name + \".csv\")), text=result.summary().as_csv()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfect Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "group_name = \"perfect_translation\"\n",
    "\n",
    "print(\"Perfect Translation\")\n",
    "print(\"\\n\\n\\nSubject: Total\")\n",
    "\n",
    "logit = sm.Logit(perfect_translation_y, X_n)\n",
    "result = logit.fit_regularized(method=\"l1\", alpha=1)\n",
    "name = \"total\"\n",
    "write_txt_file(file_path=os.path.join(output_folder_path, group_name, (name + \".csv\")), text=result.summary().as_csv())\n",
    "\n",
    "for llm_model_id in X_model_n.keys():\n",
    "    print(f\"\\n\\n\\nSubject: {llm_model_id}\")\n",
    "    x_n = X_model_n[llm_model_id]\n",
    "    y = np.array(list(map(lambda arr: arr[perfect_translation_idx], Y_model[llm_model_id])))\n",
    "    logit = sm.Logit(y, x_n)\n",
    "    result = logit.fit_regularized(method=\"l1\", alpha=1)\n",
    "    name = llm_model_id.split(\"/\")[-1]\n",
    "    write_txt_file(\n",
    "        file_path=os.path.join(output_folder_path, group_name, (name + \".csv\")), text=result.summary().as_csv()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion of Correctly Translated Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "group_name = \"correct_translation\"\n",
    "\n",
    "print(\"Proportion of Correctly Translated Elements\")\n",
    "print(\"\\n\\n\\nSubject: Total\")\n",
    "\n",
    "X_n_c = sm.add_constant(X_n_reg, prepend=False)\n",
    "mod = sm.OLS(prop_correct_y, X_n_c)\n",
    "result = mod.fit()\n",
    "name = \"total\"\n",
    "write_txt_file(file_path=os.path.join(output_folder_path, group_name, (name + \".csv\")), text=result.summary().as_csv())\n",
    "\n",
    "for llm_model_id in X_model_n_reg.keys():\n",
    "    print(f\"\\n\\n\\nSubject: {llm_model_id}\")\n",
    "    x_n = X_model_n_reg[llm_model_id]\n",
    "    x_n_c = sm.add_constant(x_n, prepend=False)\n",
    "    y = np.array(list(map(lambda arr: arr[prop_correct_idx], Y_model_reg[llm_model_id])))\n",
    "    mod = sm.OLS(y, x_n_c)\n",
    "    result = mod.fit()\n",
    "    name = llm_model_id.split(\"/\")[-1]\n",
    "    write_txt_file(\n",
    "        file_path=os.path.join(output_folder_path, group_name, (name + \".csv\")), text=result.summary().as_csv()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion of Hallucinated Elements In Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "group_name = \"hallucination\"\n",
    "\n",
    "print(\"Proportion of Hallucinated Elements\")\n",
    "print(\"\\n\\n\\nSubject: Total\")\n",
    "\n",
    "X_n_c = sm.add_constant(X_n_reg, prepend=False)\n",
    "mod = sm.OLS(prop_hallucination_y, X_n_c)\n",
    "result = mod.fit()\n",
    "name = \"total\"\n",
    "write_txt_file(file_path=os.path.join(output_folder_path, group_name, (name + \".csv\")), text=result.summary().as_csv())\n",
    "\n",
    "for llm_model_id in X_model_n_reg.keys():\n",
    "    print(f\"\\n\\n\\nSubject: {llm_model_id}\")\n",
    "    x_n = X_model_n_reg[llm_model_id]\n",
    "    x_n_c = sm.add_constant(x_n, prepend=False)\n",
    "    y = np.array(list(map(lambda arr: arr[prop_hallucination_idx], Y_model_reg[llm_model_id])))\n",
    "    mod = sm.OLS(y, x_n_c)\n",
    "    result = mod.fit()\n",
    "    name = llm_model_id.split(\"/\")[-1]\n",
    "    write_txt_file(\n",
    "        file_path=os.path.join(output_folder_path, group_name, (name + \".csv\")), text=result.summary().as_csv()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion of Missing Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "group_name = \"missing_elements\"\n",
    "\n",
    "print(\"Proportion of Hallucinated Elements\")\n",
    "print(\"\\n\\n\\nSubject: Total\")\n",
    "\n",
    "X_n_c = sm.add_constant(X_n_reg, prepend=False)\n",
    "mod = sm.OLS(prop_missing_y, X_n_c)\n",
    "result = mod.fit()\n",
    "name = \"total\"\n",
    "write_txt_file(file_path=os.path.join(output_folder_path, group_name, (name + \".csv\")), text=result.summary().as_csv())\n",
    "\n",
    "for llm_model_id in X_model_n_reg.keys():\n",
    "    print(f\"\\n\\n\\nSubject: {llm_model_id}\")\n",
    "    x_n = X_model_n_reg[llm_model_id]\n",
    "    x_n_c = sm.add_constant(x_n, prepend=False)\n",
    "    y = np.array(list(map(lambda arr: arr[prop_missing_idx], Y_model_reg[llm_model_id])))\n",
    "    mod = sm.OLS(y, x_n_c)\n",
    "    result = mod.fit()\n",
    "    name = llm_model_id.split(\"/\")[-1]\n",
    "    write_txt_file(\n",
    "        file_path=os.path.join(output_folder_path, group_name, (name + \".csv\")), text=result.summary().as_csv()\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nl2flow_runner (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
